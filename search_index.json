[["index.html", "Validation Report for adoptr package 1 Introduction 1.1 Preliminaries 1.2 Scope 1.3 Validation Scenarios 1.4 Technical Setup", " Validation Report for adoptr package Kevin Kunzmann &amp; Maximilian Pilz 2021-12-14 1 Introduction This work is licensed under the CC-BY-SA 4.0 license 1.1 Preliminaries R package validation for regulatory environments can be a tedious endeavour. The authors firmly believe that under the current regulation, there is no such thing as a ‘validated R package’: validation is by definition a process conducted by the user. This validation report merely aims at facilitating validation of adoptr as much as possible. No warranty whatsoever as to the correctness of adoptr nor the completeness of the validation report are given by the authors. We assume that the reader is familiar with the notation an theoretical background of adoptr. Otherwise, the following resources might be of help: adoptr online documentation at https://kkmann.github.io/adoptr/ paper on the theoretical background of the core adoptr functionality (Pilz et al. 2019) a general overview on adaptive designs is given in (Bauer et al. 2015) a more extensive treatment of the subject in (Wassmer and Brannath 2016). 1.2 Scope adoptr itself already makes extensive use of unittesting to ensure correctness of all implemented functions. Yet, due to constraints on the build-time for an R package, the range of scenarios covered in the unittests of adoptr is rather limited. Furthermore, the current R unittesting framework does not permit an easy generation of a human-readable report of the test cases to ascertain coverage and test quality. Therefore, adoptr splits testing in two parts: technical correctness is ensured via an extensive unittesting suit in adoptr itself (aiming to maintain a 100% code coverage). The validation report, however, runs through a wide range of possible application scenarios and ensures plausibility of results as well as consistency with existing methods wherever possible. The report itself is implemented as a collection of Rmarkdown documents allowing to show both the underlying code as well as the corresponding output in a human-readable format. The online version of the report is dynamically re-generated on a weekly basis based on the respective most current version of adoptr on CRAN. The latest result of these builds is available at https://kkmann.github.io/adoptr-validation-report/. To ensure early warning in case of any test-case failures, formal tests are implemented using the testthat package (Wickham, R Studio, and R Core Team 2018). I.e., the combination of using a unittesting framework, a continuous integration, and continuous deployment service leads to an always up-to-date validation report (build on the current R release on Linux). Any failure of the integrated formal tests will cause the build status of the validation report to switch from ‘passing’ to ‘failed’ and the respective maintainer will be notified immediately. 1.2.1 Validating a local installation of adoptr Note that, strictly speaking, the online version of the validation report only provides evidence of the correctness on the respective Travis-CI cloud virtual machine infrastructure using the respective most recent release of R and the most recent versions of the dependencies available on CRAN. In some instances it might therefore be desireable to conduct a local validaton of adoptr. To do so, one should install adoptr with the INSTALL_opts option to include tests and invoke the test suit locally via install.packages(&quot;adoptr&quot;, INSTALL_opts = c(&quot;--install-tests&quot;)) tools::testInstalledPackage(&quot;adoptr&quot;, types = c(&quot;examples&quot;, &quot;tests&quot;)) Upon passing the test suit successfully, the validation report can be build locally. To do so, first clone the entire source directory and switch to the newly created folder git clone https://github.com/kkmann/adoptr-validation-report.git cd adoptr-validation-report Make sure that all packages requied for building the report are available, i.e., install all dependencies listed in the top-level DESCRIPTION file, e.g., install.packages(c( &quot;adoptr&quot;, &quot;tidyverse&quot;, &quot;bookdown&quot;, &quot;rpact&quot;, &quot;testthat&quot;, &quot;pwr&quot; ) ) The book can then be build using the terminal command Rscript -e &#39;bookdown::render_book(&quot;index.Rmd&quot;, output_format = &quot;all&quot;)&#39; or directly from R via bookdown::render_book(&quot;index.Rmd&quot;, output_format = &quot;all&quot;) This produces a new folder _book with the html and pdf versions of the report. 1.3 Validation Scenarios 1.3.1 Scenario I: Large effect, point prior This is the default scenario. Data distribution: Two-armed trial with normally distributed test statistic Prior: \\(\\delta\\sim\\textbf{1}_{\\delta=0.4}\\) Null hypothesis: \\(\\mathcal{H}_0:\\delta \\leq 0\\) 1.3.1.1 Variant I.1: Minimizing Expected Sample Size under the Alternative Objective: \\(ESS := \\boldsymbol{E}\\big[n(X_1)\\,|\\,\\delta=0.4\\big]\\) Constraints: \\(Power := \\boldsymbol{Pr}\\big[c_2(X_1) &lt; X_2\\,|\\,\\delta=0.4\\big] \\geq 0.8\\) \\(TOER := \\boldsymbol{Pr}\\big[c_2(X_1) &lt; X_2\\,|\\,\\delta=0.0\\big] \\leq 0.025\\) Three variants: two-stage, group-sequential, one-stage. Formal tests: Number of iterations are checked against default maximum to ensure proper convergence. All three adoptr variants (two-stage, group-sequential, one-stage) comply with constraints. Internally validated by testing vs. simulated values of the power curve at respective points. Is \\(n()\\) of the optimal two-stage design monotonously decreasing on continuation area? \\(ESS\\) of optimal two-stage design is lower than \\(ESS\\) of optimal group-sequential one and that is in turn lower than the one of the optimal one-stage design. \\(ESS\\) of optimal group-sequential design is lower than \\(ESS\\) of externally computed group-sequential design using the rpact package. Are the \\(ESS\\) values obtained from simulation the same as the ones obtained by using numerical integration via adoptr::evaluate? 1.3.1.2 Variant I.2: Minimizing Expected Sample Size under the Null Hypothesis Objective: \\(ESS := \\boldsymbol{E}\\big[n(X_1)\\,|\\,\\color{red}{\\delta=0.0}\\big]\\) Constraints: \\(Power := \\boldsymbol{Pr}\\big[c_2(X_1) &lt; X_2\\,|\\,\\delta=0.4\\big] \\geq 0.8\\) \\(TOER := \\boldsymbol{Pr}\\big[c_2(X_1) &lt; X_2\\,|\\,\\delta=0.0\\big] \\leq 0.025\\) Formal tests: Number of iterations are checked against default maximum to ensure proper convergence. Validate constraint compliance by testing vs. simulated values of the power curve at respective points. \\(n()\\) of optimal design is monotonously increasing on continuation area. \\(ESS\\) of optimal two-stage design is lower than \\(ESS\\) of externally computed group-sequential design using the rpact package. Are the \\(ESS\\) values obtained from simulation the same as the ones obtained by using numerical integration via adoptr::evaluate? 1.3.1.3 Variant I.3: Condtional Power Constraint Objective: \\(ESS := \\boldsymbol{E}\\big[n(X_1)\\,|\\,\\delta=0.4\\big]\\) Constraints: \\(Power := \\boldsymbol{Pr}\\big[c_2(X_1) &lt; X_2\\,|\\,\\delta=0.4\\big] \\geq 0.8\\) \\(TOER := \\boldsymbol{Pr}\\big[c_2(X_1) &lt; X_2\\,|\\,\\delta=0.0\\big] \\leq 0.025\\) \\(CP := \\color{red}{\\boldsymbol{Pr}\\big[c_2(X_1) &lt; X_2\\,|\\,\\delta=0.4, X_1 = x_1\\big] \\geq 0.7}\\) for all \\(x_1\\in(c_1^f, c_1^e)\\) Formal tests: Number of iterations are checked against default maximum to ensure proper convergence. Check \\(Power\\) and \\(TOER\\) constraints with simulation. Check \\(CP\\) constraint on 25 different values of \\(x_1\\) in \\([c_1^f, c_1^e]\\) Are the \\(CP\\) values at the 25 test-pivots obtained from simulation the same as the ones obtained by using numerical integration via adoptr::evaluate? Is \\(ESS\\) of optimal two-stage design with \\(CP\\) constraint higher than \\(ESS\\) of optimal two-stage design without this constraint? 1.3.2 Scenario II: Large effect, Gaussian prior Similar scope to Scenario I, but with a continuous Gaussian prior on \\(\\delta\\). Data distribution: Two-armed trial with normally distributed test statistic Prior: \\(\\delta\\sim\\mathcal{N}(0.4, .3)\\) Null hypothesis: \\(\\mathcal{H}_0:\\delta \\leq 0\\) 1.3.2.1 Variant II.1: Minimizing Expected Sample Size Objective: \\(ESS := \\boldsymbol{E}\\big[n(X_1)\\big]\\) Constraints: \\(Power := \\boldsymbol{Pr}\\big[c_2(X_1) &lt; X_2\\,|\\,\\delta&gt; 0.0\\big] \\geq 0.8\\) \\(TOER := \\boldsymbol{Pr}\\big[c_2(X_1) &lt; X_2\\,|\\,\\delta=0.0\\big] \\leq 0.025\\) Three variants: two-stage, group-sequential, one-stage. Formal tests: Number of iterations are checked against default maximum to ensure proper convergence. All designs comply with type one error rate constraints (tested via simulation). \\(ESS\\) of optimal two-stage design is lower than \\(ESS\\) of optimal group-sequential one and that is in turn lower than the one of the optimal one-stage design. 1.3.2.2 Variant II.2: Minimizing Expected Sample Size under the Null hypothesis Objective: \\(ESS := \\boldsymbol{E}\\big[n(X_1)\\,|\\,\\color{red}{\\delta\\leq 0}\\big]\\) Constraints: \\(Power := \\boldsymbol{Pr}\\big[c_2(X_1) &lt; X_2\\,|\\,\\delta&gt; 0.0\\big] \\geq 0.8\\) \\(TOER := \\boldsymbol{Pr}\\big[c_2(X_1) &lt; X_2\\,|\\,\\delta=0.0\\big] \\leq 0.025\\) Formal tests: Number of iterations are checked against default maximum to ensure proper convergence. Does the design comply with \\(TOER\\) constraint (via simulation)? Is \\(ESS\\) lower than expected sample size under the null hypothesis for the optimal two stage design from Variant II-1? 1.3.2.3 Variant II.3: Condtional Power Constraint Objective: \\(ESS := \\boldsymbol{E}\\big[n(X_1)\\big]\\) Constraints: \\(Power := \\boldsymbol{Pr}\\big[c_2(X_1) &lt; X_2\\,|\\,\\delta&gt;0.0\\big] \\geq 0.8\\) \\(TOER := \\boldsymbol{Pr}\\big[c_2(X_1) &lt; X_2\\,|\\,\\delta=0.0\\big] \\leq 0.025\\) \\(CP := \\color{red}{\\boldsymbol{Pr}\\big[c_2(X_1) &lt; X_2\\,|\\,\\delta&gt; 0.0, X_1 = x_1\\big] \\geq 0.7}\\) for all \\(x_1\\in(c_1^f, c_1^e)\\) Formal tests: Number of iterations are checked against default maximum to ensure proper convergence. Check \\(TOER\\) constraint with simulation. Check \\(CP\\) constraint on three different values of \\(x_1\\) in \\((c_1^f, c_1^e)\\) Is \\(ESS\\) of optimal two-stage design with \\(CP\\) constraint higher than \\(ESS\\) of optimal two-stage design without the constraint? 1.3.3 Scenario III: Large effect, uniform prior Data distribution: Two-armed trial with normally distributed test statistic Prior: sequence of uniform distributions \\(\\delta\\sim\\operatorname{Unif}(0.4 - \\Delta_i, 0.4 + \\Delta_i)\\) around \\(0.4\\) with \\(\\Delta_i=(3 - i)/10\\) for \\(i=0\\ldots 3\\). I.e., for \\(\\Delta_3=0\\) reduces to a point prior on \\(\\delta=0.4\\). Null hypothesis: \\(\\mathcal{H}_0:\\delta \\leq 0\\) 1.3.3.1 Variant III.1: Convergence under Prior Concentration Objective: \\(ESS := \\boldsymbol{E}\\big[n(X_1)\\big]\\) Constraints: \\(Power := \\boldsymbol{Pr}\\big[c_2(X_1) &lt; X_2\\,|\\,\\delta&gt;0.0\\big] \\geq 0.8\\) \\(TOER := \\boldsymbol{Pr}\\big[c_2(X_1) &lt; X_2\\,|\\,\\delta=0.0\\big] \\leq 0.025\\) Formal tests: Number of iterations are checked against default maximum to ensure proper convergence. Simulated type one error rate is compared to \\(TOER\\) constraint for each design. \\(ESS\\) decreases with prior variance. Additionally, the designs are compared graphically. Inspect the plot to see convergence pattern. 1.3.4 Scenario IV: Smaller effect size, larger trials 1.3.4.1 Variant IV.1: Minimizing Expected Sample Size under the Alternative Objective: \\(ESS := \\boldsymbol{E}\\big[n(X_1)\\,|\\,\\delta=0.2\\big]\\) Constraints: \\(Power := \\boldsymbol{Pr}\\big[c_2(X_1) &lt; X_2\\,|\\,\\delta=0.2\\big] \\geq 0.8\\) \\(TOER := \\boldsymbol{Pr}\\big[c_2(X_1) &lt; X_2\\,|\\,\\delta=0.0\\big] \\leq 0.025\\) Three variants: two-stage, group-sequential, one-stage. Formal tests: Number of iterations are checked against default maximum to ensure proper convergence. All three adoptr variants (two-stage, group-sequential, one-stage) comply with constraints. Internally validated by testing vs. simulated values of the power curve at respective points. \\(ESS\\) of optimal two-stage design is lower than \\(ESS\\) of optimal group-sequential one and that is in turn lower than the one of the optimal one-stage design. \\(ESS\\) of optimal group-sequential design is lower than \\(ESS\\) of externally computed group-sequential design using the rpact package. Are the \\(ESS\\) values obtained from simulation the same as the ones obtained by using numerical integration via adoptr::evaluate? Is \\(n()\\) of the optimal two-stage design monotonously decreasing on continuation area? 1.3.4.2 Variant IV.2: Increasing Power Objective: \\(ESS := \\boldsymbol{E}\\big[n(X_1)\\,|\\,\\delta=0.2\\big]\\) Constraints: \\(Power := \\boldsymbol{Pr}\\big[c_2(X_1) &lt; X_2\\,|\\,\\delta=0.2\\big] \\geq \\color{red}{0.9}\\) \\(TOER := \\boldsymbol{Pr}\\big[c_2(X_1) &lt; X_2\\,|\\,\\delta=0.0\\big] \\leq 0.025\\) Three variants: two-stage, group-sequential, one-stage. Formal tests: Number of iterations are checked against default maximum to ensure proper convergence. Does the design respect all constraints (via simulation)? \\(ESS\\) of optimal two-stage design is lower than \\(ESS\\) of optimal group-sequential one and that is in turn lower than the one of the optimal one-stage design. \\(ESS\\) of optimal group-sequential design is lower than \\(ESS\\) of externally computed group-sequential design using the rpact package. Are the \\(ESS\\) values obtained from simulation the same as the ones obtained by using numerical integration via adoptr::evaluate? Is \\(n()\\) of the optimal two-stage design monotonously decreasing on continuation area? 1.3.4.3 Variant IV.3: Increasing Maximal Type One Error Rate Objective: \\(ESS := \\boldsymbol{E}\\big[n(X_1)\\,|\\,\\delta=0.2\\big]\\) Constraints: \\(Power := \\boldsymbol{Pr}\\big[c_2(X_1) &lt; X_2\\,|\\,\\delta=0.2\\big] \\geq 0.8\\) \\(TOER := \\boldsymbol{Pr}\\big[c_2(X_1) &lt; X_2\\,|\\,\\delta=0.0\\big] \\leq \\color{red}{0.05}\\) Three variants: two-stage, group-sequential, one-stage. Formal tests: Number of iterations are checked against default maximum to ensure proper convergence. Does the design respect all constraints (via simulation)? \\(ESS\\) of optimal two-stage design is lower than \\(ESS\\) of optimal group-sequential one and that is in turn lower than the one of the optimal one-stage design. \\(ESS\\) of optimal group-sequential design is lower than \\(ESS\\) of externally computed group-sequential design using the rpact package. Are the \\(ESS\\) values obtained from simulation the same as the ones obtained by using numerical integration via adoptr::evaluate? Is \\(n()\\) of the optimal two-stage design monotonously decreasing on continuation area? 1.3.5 Scenario V: Single-arm design, medium effect size Data distribution: trial with normally distributed test statistic Prior: \\(\\delta\\sim\\delta_{0.3}\\) Null hypothesis: \\(\\mathcal{H}_0:\\delta \\leq 0\\) 1.3.5.1 Variant V.1: Sensitivity to Integration Order Objective: \\(ESS := \\boldsymbol{E}\\big[n(X_1)\\,|\\,\\delta=0.3\\big]\\) Constraints: \\(Power := \\boldsymbol{Pr}\\big[c_2(X_1) &lt; X_2\\,|\\,\\color{red}{\\delta=0.3}\\big] \\geq 0.8\\) \\(TOER := \\boldsymbol{Pr}\\big[c_2(X_1) &lt; X_2\\,|\\,\\delta=0.0\\big] \\leq 0.025\\) Three variants: integration order 5, 8, 11 two-stage designs. Formal tests: Do all designs converge within the respective iteration limit? Do all designs respect all constraints (via simulation)? 1.3.5.2 Variant V.2: Utility Maximization Objective: \\(\\lambda\\, Power - ESS := \\lambda\\, \\boldsymbol{Pr}\\big[c_2(X_1) &lt; X_2\\,|\\,\\delta=0.3\\big] - \\boldsymbol{E}\\big[n(X_1)\\,|\\,\\delta=0.3\\big].\\) for \\(\\lambda = 100\\) and \\(200\\) Constraints: \\(TOER := \\boldsymbol{Pr}\\big[c_2(X_1) &lt; X_2\\,|\\,\\delta=0.0\\big] \\leq 0.025\\) Formal tests: Number of iterations are checked against default maximum to ensure proper convergence. Do both designs respect the type one error rate constraint (via simulation)? Is the power of the design with larger \\(\\lambda\\) larger? 1.3.5.3 Variant V.3: \\(n_1\\) penalty Objective: \\(ESS := \\boldsymbol{E}\\big[n(X_1)\\,|\\,\\delta=0.3\\big] + \\lambda \\, n_1\\) for \\(\\lambda = 0.05\\) and \\(0.2\\). Constraints: \\(TOER := \\boldsymbol{Pr}\\big[c_2(X_1) &lt; X_2\\,|\\,\\delta=0.0\\big] \\leq 0.025\\) \\(Power := \\boldsymbol{Pr}\\big[c_2(X_1) &lt; X_2\\,|\\,\\delta=0.3\\big] \\geq 0.8\\) Formal tests: Number of iterations are checked against default maximum to ensure proper convergence. Do both designs respect the type one error rate and power constraints (via simulation)? Is \\(n_1\\) for the optimal design smaller than the order-5 design in V.1? 1.3.5.4 Variant V.4: \\(n_2\\) penalty Objective: \\(ESS := \\boldsymbol{E}\\big[n(X_1)\\,|\\,\\delta=0.3\\big] + \\lambda\\) AverageN2 for \\(\\lambda = 0.01\\) and \\(0.1\\). Constraints: \\(TOER := \\boldsymbol{Pr}\\big[c_2(X_1) &lt; X_2\\,|\\,\\delta=0.0\\big] \\leq 0.025\\) \\(Power := \\boldsymbol{Pr}\\big[c_2(X_1) &lt; X_2\\,|\\,\\delta=0.3\\big] \\geq 0.8\\) Formal tests: Number of iterations are checked against default maximum to ensure proper convergence. Do both designs respect the type one error rate and power constraints (via simulation)? Is the AverageN2 for the optimal design smaller than for the order-5 design in V.1? 1.3.6 Scenario VI: Binomial distribution This scenario investigates the implementation of the binomial distribution. Data distribution: Two-armed trial with binomial distributed outcomes. Thus \\(\\delta := p_E - p_C\\) refers to the rate difference here. The control rate is assumed to equal \\(p_C = 0.3\\). Prior: \\(\\delta\\sim\\textbf{1}_{\\delta=0.2}\\) Null hypothesis: \\(\\mathcal{H}_0:\\delta \\leq 0\\) 1.3.6.1 Variant VI.1: Minimizing Expected Sample Size under the Alternative Objective: \\(ESS := \\boldsymbol{E}\\big[n(X_1)\\,|\\,\\delta=0.2\\big]\\) Constraints: \\(Power := \\boldsymbol{Pr}\\big[c_2(X_1) &lt; X_2\\,|\\,\\delta=0.2\\big] \\geq 0.9\\) \\(TOER := \\boldsymbol{Pr}\\big[c_2(X_1) &lt; X_2\\,|\\,\\delta=0.0\\big] \\leq 0.025\\) Three variants: two-stage, group-sequential, one-stage. Formal tests: Number of iterations are checked against default maximum to ensure proper convergence. All three adoptr variants (two-stage, group-sequential, one-stage) comply with constraints. Internally validated by testing vs. simulated values of the power curve at respective points. \\(ESS\\) of optimal two-stage design is lower than \\(ESS\\) of optimal group-sequential one and that is in turn lower than the one of the optimal one-stage design. \\(ESS\\) of optimal group-sequential design is lower than \\(ESS\\) of externally computed group-sequential design using the rpact package. Are the \\(ESS\\) values obtained from simulation the same as the ones obtained by using numerical integration via adoptr::evaluate? 1.4 Technical Setup All scenarios are run in a single, shared R session. Required packages are loaded here, the random seed is defined and set centrally, and the default number of iteration is increased to make sure that all scenarios converge properly. Additionally R scripts with convenience functions are sourced here as well. There are three additional functions for this report. rpact_design creates a two-stage design via the package rpact (Wassmer and Pahlke 2018) in the notation of adoptr. sim_pr_reject and sim_n allow to simulate rejection probabilities and expected sample sizes respectively by the adoptr routine simulate. Furthermore, global tolerances for the validation are set. For error rates, a relative deviation of \\(1\\%\\) from the target value is accepted. (Expected) Sample sizes deviations are more liberally accepted up to an absolute deviation of \\(0.5\\). library(adoptr) library(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ── ## ✔ ggplot2 3.3.5 ✔ purrr 0.3.4 ## ✔ tibble 3.1.6 ✔ dplyr 1.0.7 ## ✔ tidyr 1.1.4 ✔ stringr 1.4.0 ## ✔ readr 2.1.1 ✔ forcats 0.5.1 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() ## ✖ dplyr::n() masks adoptr::n() library(rpact) library(pwr) library(testthat) ## ## Attaching package: &#39;testthat&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## matches ## The following object is masked from &#39;package:purrr&#39;: ## ## is_null ## The following objects are masked from &#39;package:readr&#39;: ## ## edition_get, local_edition ## The following object is masked from &#39;package:tidyr&#39;: ## ## matches ## The following object is masked from &#39;package:adoptr&#39;: ## ## expectation library(tinytex) # load custom functions in folder subfolder &#39;/R&#39; for (nm in list.files(&quot;R&quot;, pattern = &quot;\\\\.[RrSsQq]$&quot;)) source(file.path(&quot;R&quot;, nm)) # define seed value seed &lt;- 42 # define absolute tolerance for error rates tol &lt;- 0.01 # define absolute tolerance for sample sizes tol_n &lt;- 0.5 # define custom tolerance and iteration limit for nloptr opts = list( algorithm = &quot;NLOPT_LN_COBYLA&quot;, xtol_rel = 1e-5, maxeval = 100000 ) References "],["scenarioI.html", "2 Scenario I: large effect, point prior 2.1 Details 2.2 Variant I-1: Minimizing Expected Sample Size under Point Prior 2.3 Variant I-2: Minimizing Expected Sample Size under Null Hypothesis 2.4 Variant I-3: Conditional Power Constraint 2.5 Plot Two-Stage Designs", " 2 Scenario I: large effect, point prior 2.1 Details In this scenario, a classical two-arm trial with normal test statistic and known variance (w.l.o.g. variance of the test statistic is 1). This situation corresponds to a classical \\(z\\)-test for a difference in population means. The null hypothesis is no population mean difference, i.e. \\(\\mathcal{H}_0:\\delta \\leq 0\\). An alternative effect size of \\(\\delta = 0.4\\) with point prior distribution is assumed. Across all variants in this scenario, the one-sided maximal type one error rate is restricted to \\(\\alpha=0.025\\) and the power at the point alternative of \\(\\delta=0.4\\) must be at least \\(0.8\\). # data distribution and hypotheses datadist &lt;- Normal(two_armed = TRUE) H_0 &lt;- PointMassPrior(.0, 1) prior &lt;- PointMassPrior(.4, 1) # define constraints alpha &lt;- 0.025 min_power &lt;- 0.8 toer_cnstr &lt;- Power(datadist, H_0) &lt;= alpha pow_cnstr &lt;- Power(datadist, prior) &gt;= min_power 2.2 Variant I-1: Minimizing Expected Sample Size under Point Prior 2.2.1 Objective Firstly, expected sample size under the alternative (point prior) is minimized, i.e., \\(\\boldsymbol{E}\\big[n(\\mathcal{D})\\big]\\). ess &lt;- ExpectedSampleSize(datadist, prior) 2.2.2 Constraints No additional constraints besides type one error rate and power are considered in this variant. 2.2.3 Initial Designs For this example, the optimal one-stage, group-sequential, and generic two-stage designs are computed. The initial design for the one-stage case is determined heuristically (cf. Scenario III where another initial design is applied on the same situation for stability of initial values). Both the group sequential and the generic two-stage designs are optimized starting from the corresponding group-sequential design as computed by the rpact package. order &lt;- 7L # data frame of initial designs tbl_designs &lt;- tibble( type = c(&quot;one-stage&quot;, &quot;group-sequential&quot;, &quot;two-stage&quot;), initial = list( OneStageDesign(200, 2.0), rpact_design(datadist, 0.4, 0.025, 0.8, TRUE, order), TwoStageDesign(rpact_design(datadist, 0.4, 0.025, 0.8, TRUE, order))) ) The order of integration is set to 7. 2.2.4 Optimization tbl_designs &lt;- tbl_designs %&gt;% mutate( optimal = purrr::map(initial, ~minimize( ess, subject_to( toer_cnstr, pow_cnstr ), initial_design = ., opts = opts)) ) 2.2.5 Test Cases To avoid improper solutions, it is first verified that the maximum number of iterations was not exceeded in any of the three cases. tbl_designs %&gt;% transmute( type, iterations = purrr::map_int(tbl_designs$optimal, ~.$nloptr_return$iterations) ) %&gt;% {print(.); .} %&gt;% {testthat::expect_true(all(.$iterations &lt; opts$maxeval))} ## # A tibble: 3 × 2 ## type iterations ## &lt;chr&gt; &lt;int&gt; ## 1 one-stage 24 ## 2 group-sequential 1447 ## 3 two-stage 4268 Next, the type one error rate and power constraints are verified for all three designs by simulation: tbl_designs %&gt;% transmute( type, toer = purrr::map(tbl_designs$optimal, ~sim_pr_reject(.[[1]], .0, datadist)$prob), power = purrr::map(tbl_designs$optimal, ~sim_pr_reject(.[[1]], .4, datadist)$prob) ) %&gt;% unnest(., cols = c(toer, power)) %&gt;% {print(.); .} %&gt;% { testthat::expect_true(all(.$toer &lt;= alpha * (1 + tol))) testthat::expect_true(all(.$power &gt;= min_power * (1 - tol))) } ## # A tibble: 3 × 3 ## type toer power ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 one-stage 0.0251 0.799 ## 2 group-sequential 0.0250 0.800 ## 3 two-stage 0.0250 0.799 The \\(n_2\\) function of the optimal two-stage design is expected to be monotonously decreasing: expect_true( all(diff( # get optimal two-stage design n2 pivots tbl_designs %&gt;% filter(type == &quot;two-stage&quot;) %&gt;% {.[[&quot;optimal&quot;]][[1]]$design@n2_pivots} ) &lt; 0) ) Since the degrees of freedom of the three design classes are ordered as ‘two-stage’ &gt; ‘group-sequential’ &gt; ‘one-stage’, the expected sample sizes (under the alternative) should be ordered in reverse (‘two-stage’ smallest). Additionally, expected sample sizes under both null and alternative are computed both via evaluate() and simulation-based. ess0 &lt;- ExpectedSampleSize(datadist, H_0) tbl_designs %&gt;% mutate( ess = map_dbl(optimal, ~evaluate(ess, .$design) ), ess_sim = map_dbl(optimal, ~sim_n(.$design, .4, datadist)$n ), ess0 = map_dbl(optimal, ~evaluate(ess0, .$design) ), ess0_sim = map_dbl(optimal, ~sim_n(.$design, .0, datadist)$n ) ) %&gt;% {print(.); .} %&gt;% { # sim/evaluate same under alternative? testthat::expect_equal(.$ess, .$ess_sim, tolerance = tol_n, scale = 1) # sim/evaluate same under null? testthat::expect_equal(.$ess0, .$ess0_sim, tolerance = tol_n, scale = 1) # monotonicity with respect to degrees of freedom testthat::expect_true(all(diff(.$ess) &lt; 0)) } ## # A tibble: 3 × 7 ## type initial optimal ess ess_sim ess0 ess0_sim ## &lt;chr&gt; &lt;list&gt; &lt;list&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 one-stage &lt;OnStgDsg&gt; &lt;adptrOpR [3]&gt; 98 98 98 98 ## 2 group-sequential &lt;GrpSqntD&gt; &lt;adptrOpR [3]&gt; 80.9 80.9 68.5 68.5 ## 3 two-stage &lt;TwStgDsg&gt; &lt;adptrOpR [3]&gt; 79.7 79.7 68.9 68.9 The expected sample size under the alternative must be lower or equal than the expected sample size of the initial rpact group-sequential design that is based on the inverse normal combination test. testthat::expect_lte( evaluate(ess, tbl_designs %&gt;% filter(type == &quot;group-sequential&quot;) %&gt;% pull(optimal) %&gt;% .[[1]] %&gt;% .$design ), evaluate(ess, tbl_designs %&gt;% filter(type == &quot;group-sequential&quot;) %&gt;% pull(initial) %&gt;% .[[1]] ) ) 2.3 Variant I-2: Minimizing Expected Sample Size under Null Hypothesis 2.3.1 Objective Expected sample size under the null hypothesis prior is minimized, i.e., ess0. 2.3.2 Constraints The constraints remain unchanged from the base case. 2.3.3 Initial Design Since optimization under the null favours an entirely different (monotonically increasing) sample size function, and thus also a different shape of the \\(c_2\\) function, the rpact initial design is a suboptimal starting point. Instead, we start with a constant \\(c_2\\) function by heuristically setting it to \\(2\\) on the continuation area. Also, optimizing under the null favours extremely conservative boundaries for early efficacy stopping and we thus impose as fairly liberal upper bound of \\(3\\) for early efficacy stopping. init_design_h0 &lt;- tbl_designs %&gt;% filter(type == &quot;two-stage&quot;) %&gt;% pull(initial) %&gt;% .[[1]] init_design_h0@c2_pivots &lt;- rep(2, order) ub_design &lt;- TwoStageDesign( 3 * init_design_h0@n1, 2, 3, rep(300, order), rep(3.0, order) ) 2.3.4 Optimization The optimal two-stage design is computed. opt_h0 &lt;- minimize( ess0, subject_to( toer_cnstr, pow_cnstr ), initial_design = init_design_h0, upper_boundary_design = ub_design, opts = opts ) 2.3.5 Test Cases Make sure that the optimization algorithm converged within the set maximum number of iterations: opt_h0$nloptr_return$iterations %&gt;% {print(.); .} %&gt;% {testthat::expect_true(. &lt; opts$maxeval)} ## [1] 17412 The \\(n_2\\) function of the optimal two-stage design is expected to be monotonously increasing. expect_true( all(diff(opt_h0$design@n2_pivots) &gt; 0) ) Next, the type one error rate and power constraints are tested. tbl_performance &lt;- tibble( delta = c(.0, .4) ) %&gt;% mutate( power = map( delta, ~evaluate( Power(datadist, PointMassPrior(., 1)), opt_h0$design) ), power_sim = map( delta, ~sim_pr_reject(opt_h0$design, ., datadist)$prob), ess = map( delta, ~evaluate(ExpectedSampleSize( datadist, PointMassPrior(., 1) ), opt_h0$design) ), ess_sim = map( delta, ~sim_n(opt_h0$design, . ,datadist)$n ) ) %&gt;% unnest(., cols = c(power, power_sim, ess, ess_sim)) print(tbl_performance) ## # A tibble: 2 × 5 ## delta power power_sim ess ess_sim ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 0.0250 0.0250 57.2 57.3 ## 2 0.4 0.802 0.802 118. 118. testthat::expect_lte( tbl_performance %&gt;% filter(delta == 0) %&gt;% pull(power_sim), alpha * (1 + tol) ) testthat::expect_gte( tbl_performance %&gt;% filter(delta == 0.4) %&gt;% pull(power_sim), min_power * (1 - tol) ) # make sure that evaluate() leads to same results testthat::expect_equal( tbl_performance$power, tbl_performance$power_sim, tol = tol, scale = 1 ) testthat::expect_equal( tbl_performance$ess, tbl_performance$ess_sim, tol = tol_n, scale = 1 ) The expected sample size under the null must be lower or equal than the expected sample size of the initial rpact group-sequential design. testthat::expect_gte( evaluate(ess0, tbl_designs %&gt;% filter(type == &quot;two-stage&quot;) %&gt;% pull(initial) %&gt;% .[[1]] ), evaluate(ess0, opt_h0$design) ) 2.4 Variant I-3: Conditional Power Constraint 2.4.1 Objective Same as in I-1, i.e., expected sample size under the alternative point prior is minimized. 2.4.2 Constraints Besides the previous global type one error rate and power constraints, an additional constraint on conditional power is imposed. cp &lt;- ConditionalPower(datadist, prior) cp_cnstr &lt;- cp &gt;= .7 2.4.3 Initial Design The same initial (generic two-stage) design as in I-1 is used. 2.4.4 Optimization opt_cp &lt;- minimize( ess, subject_to( toer_cnstr, pow_cnstr, cp_cnstr # new constraint ), initial_design = tbl_designs %&gt;% filter(type == &quot;two-stage&quot;) %&gt;% pull(initial) %&gt;% .[[1]], opts = opts ) 2.4.5 Test Cases Check if the optimization algorithm converged. opt_cp$nloptr_return$iterations %&gt;% {print(.); .} %&gt;% {testthat::expect_true(. &lt; opts$maxeval)} ## [1] 4224 Check constraints. tbl_performance &lt;- tibble( delta = c(.0, .4) ) %&gt;% mutate( power = map( delta, ~evaluate( Power(datadist, PointMassPrior(., 1)), opt_cp$design) ), power_sim = map( delta, ~sim_pr_reject(opt_cp$design, ., datadist)$prob), ess = map( delta, ~evaluate(ExpectedSampleSize( datadist, PointMassPrior(., 1) ), opt_cp$design) ), ess_sim = map( delta, ~sim_n(opt_cp$design, . ,datadist)$n ) ) %&gt;% unnest(., cols = c(power, power_sim, ess, ess_sim)) print(tbl_performance) ## # A tibble: 2 × 5 ## delta power power_sim ess ess_sim ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 0.0250 0.0250 68.9 69.0 ## 2 0.4 0.799 0.799 79.8 79.8 testthat::expect_lte( tbl_performance %&gt;% filter(delta == 0) %&gt;% pull(power_sim), alpha * (1 + tol) ) testthat::expect_gte( tbl_performance %&gt;% filter(delta == 0.4) %&gt;% pull(power_sim), min_power * (1 - tol) ) # make sure that evaluate() leads to same results testthat::expect_equal( tbl_performance$power, tbl_performance$power_sim, tol = tol, scale = 1 ) testthat::expect_equal( tbl_performance$ess, tbl_performance$ess_sim, tol = tol_n, scale = 1 ) The conditional power constraint is evaluated and tested on a grid over the continuation region (both simulated an via numerical integration). tibble( x1 = seq(opt_cp$design@c1f, opt_cp$design@c1e, length.out = 25), cp = map_dbl(x1, ~evaluate(cp, opt_cp$design, .)), cp_sim = map_dbl(x1, function(x1) { x2 &lt;- simulate(datadist, 10^6, n2(opt_cp$design, x1), .4, 42) rej &lt;- ifelse(x2 &gt; c2(opt_cp$design, x1), 1, 0) return(mean(rej)) }) ) %&gt;% {print(.); .} %&gt;% { testthat::expect_true(all(.$cp &gt;= 0.7 * (1 - tol))) testthat::expect_true(all(.$cp_sim &gt;= 0.7 * (1 - tol))) testthat::expect_true(all(abs(.$cp - .$cp_sim) &lt;= tol)) } ## # A tibble: 25 × 3 ## x1 cp cp_sim ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.810 0.701 0.701 ## 2 0.872 0.698 0.699 ## 3 0.934 0.701 0.701 ## 4 0.995 0.698 0.698 ## 5 1.06 0.700 0.700 ## 6 1.12 0.702 0.703 ## 7 1.18 0.699 0.699 ## 8 1.24 0.703 0.703 ## 9 1.30 0.713 0.713 ## 10 1.37 0.718 0.718 ## # … with 15 more rows Finally, the expected sample size under the alternative prior should be larger than in the case without the constraint I-1. testthat::expect_gte( evaluate(ess, opt_cp$design), evaluate( ess, tbl_designs %&gt;% filter(type == &quot;two-stage&quot;) %&gt;% pull(optimal) %&gt;% .[[1]] %&gt;% .$design ) ) 2.5 Plot Two-Stage Designs The following figure shows the three optimal two-stage designs side by side. The effect of the conditional power constraint (CP not below 0.7) is clearly visible and the very different characteristics between optimizing under the null or the alternative are clearly visible. "],["scenarioII.html", "3 Scenario II: large effect, Gaussian prior 3.1 Details 3.2 Variant II-1: Minimizing Expected Sample Size under Point Prior 3.3 Variant II-2: Minimizing Expected Sample Size under Null Hypothesis 3.4 Variant II-3: Conditional Power Constraint 3.5 Plot Two-Stage Designs", " 3 Scenario II: large effect, Gaussian prior 3.1 Details In this scenario, we revisit the case from Scenario I, but are not assuming a point prior any more. Instead, a Gaussian prior with mean \\(\\vartheta = 0.4\\) and variance \\(\\tau^2 = 0.2^2\\) on the effect size is assumed, i.e.  \\(\\delta \\sim \\mathcal{N} (0.4, 0.2^2)\\). In order to fulfill regulatory considerations, the type one error rate is still protected under the point prior \\(\\delta = 0\\) at the level of significance \\(\\alpha = 0.025\\). The power constraint, however, needs to be modified. It is not senseful to compute the power as rejection probability under the full prior, because effect sizes less than a minimal clinically relevant effect do not show (sufficient) evidence againt the null hypothesis. Therefore, we assume a minimal clinically relevant effect size \\(\\delta = 0.0\\) and condition the prior on values \\(\\delta &gt; 0\\) to compute expected power. In the following, the expected power should be at least \\(0.8\\). # data distribution and priors datadist &lt;- Normal(two_armed = TRUE) H_0 &lt;- PointMassPrior(.0, 1) prior &lt;- ContinuousPrior(function(delta) dnorm(delta, mean = .4, sd = .2), support = c(-5, 5), tighten_support = TRUE) # define constraints on type one error rate and expected power alpha &lt;- 0.025 min_epower &lt;- 0.8 toer_cnstr &lt;- Power(datadist, H_0) &lt;= alpha epow_cnstr &lt;- Power(datadist, condition(prior, c(0.0, prior@support[2]))) &gt;= min_epower 3.2 Variant II-1: Minimizing Expected Sample Size under Point Prior 3.2.1 Objective Expected sample size under the full prior is minimized, i.e., \\(\\boldsymbol{E}\\big[n(\\mathcal{D})\\big]\\). ess &lt;- ExpectedSampleSize(datadist, prior) 3.2.2 Constraints No additional constraints are considered in this variant. 3.2.3 Initial Design For this example, the optimal one-stage, group-sequential, and generic two-stage designs are computed. While the initial design for the one-stage case is determined heuristically, both the group sequential and the generic two-stage designs are optimized starting from the a group-sequential design that is computed by the rpact package to fulfill the type one error rate constraint and that fulfills the power constraint at an effect size of \\(\\delta = 0.3\\). order &lt;- 5L # data frame of initial designs tbl_designs &lt;- tibble( type = c(&quot;one-stage&quot;, &quot;group-sequential&quot;, &quot;two-stage&quot;), initial = list( OneStageDesign(250, 2.0), rpact_design(datadist, 0.3, 0.025, 0.8, TRUE, order), TwoStageDesign(rpact_design(datadist, 0.3, 0.025, 0.8, TRUE, order))) ) The order of integration is set to 5. 3.2.4 Optimization For all these three initial designs, the resulting optimal designs are computed. tbl_designs &lt;- tbl_designs %&gt;% mutate( optimal = purrr::map(initial, ~minimize( ess, subject_to( toer_cnstr, epow_cnstr ), initial_design = ., opts = opts)) ) 3.2.5 Test Cases Firstly, it is checked that the maximum number of iterations was not reached in all these cases. tbl_designs %&gt;% transmute( type, iterations = purrr::map_int(tbl_designs$optimal, ~.$nloptr_return$iterations) ) %&gt;% {print(.); .} %&gt;% {testthat::expect_true(all(.$iterations &lt; opts$maxeval))} ## # A tibble: 3 × 2 ## type iterations ## &lt;chr&gt; &lt;int&gt; ## 1 one-stage 26 ## 2 group-sequential 720 ## 3 two-stage 1965 Since type one error rate is defined under the point effect size \\(\\delta=0\\), the type one error rate constraint can be tested for all three optimal designs. tbl_designs %&gt;% transmute( type, toer = purrr::map(tbl_designs$optimal, ~sim_pr_reject(.[[1]], .0, datadist)$prob) ) %&gt;% unnest(., cols = c(toer)) %&gt;% {print(.); .} %&gt;% { testthat::expect_true(all(.$toer &lt;= alpha * (1 + tol))) } ## # A tibble: 3 × 2 ## type toer ## &lt;chr&gt; &lt;dbl&gt; ## 1 one-stage 0.0251 ## 2 group-sequential 0.0250 ## 3 two-stage 0.0249 Since the optimal two-stage design is more flexible than the optimal group-sequential design (constant \\(n_2\\) function) and this is more flexible than the optimal one-stage design (no second stage), the expected sample sizes under the prior should be ordered in the opposite way. Additionally, expected sample sizes under the null hypothesis are computed both via evaluate() and simulation-based. essh0 &lt;- ExpectedSampleSize(datadist, H_0) tbl_designs %&gt;% mutate( ess = map_dbl(optimal, ~evaluate(ess, .$design) ), essh0 = map_dbl(optimal, ~evaluate(essh0, .$design) ), essh0_sim = map_dbl(optimal, ~sim_n(.$design, .0, datadist)$n ) ) %&gt;% {print(.); .} %&gt;% { # sim/evaluate same under null? testthat::expect_equal(.$essh0, .$essh0_sim, tolerance = tol_n, scale = 1) # monotonicity with respect to degrees of freedom testthat::expect_true(all(diff(.$ess) &lt; 0)) } ## # A tibble: 3 × 6 ## type initial optimal ess essh0 essh0_sim ## &lt;chr&gt; &lt;list&gt; &lt;list&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 one-stage &lt;OnStgDsg&gt; &lt;adptrOpR [3]&gt; 165 165 165 ## 2 group-sequential &lt;GrpSqntD&gt; &lt;adptrOpR [3]&gt; 117. 115. 115. ## 3 two-stage &lt;TwStgDsg&gt; &lt;adptrOpR [3]&gt; 115. 119. 119. 3.3 Variant II-2: Minimizing Expected Sample Size under Null Hypothesis 3.3.1 Objective Expected sample size conditioned on negative effect sizes is minimized, i.e., ess_0 &lt;- ExpectedSampleSize(datadist, condition(prior, c(bounds(prior)[1], 0))) 3.3.2 Constraints No additional constraints besides type one error rate and expected power are considered in this variant. 3.3.3 Initial Design As in Variant I.2 another initial design is more appropriate for optimization under the null hypothesis. In this situation, one may expect a different (increasing) sample size function, and thus also a different shape of the \\(c_2\\) function. Therefore, the rpact initial design is a suboptimal starting point. Instead, we start with a constant \\(c_2\\) function by heuristically setting it to \\(2\\) on the continuation area. Since optimization under the null hypothesis favours extremely conservative boundaries for early efficacy stopping we impose as quite liberal upper bound of \\(3\\) for early efficacy stopping. init_design_h0 &lt;- tbl_designs %&gt;% filter(type == &quot;two-stage&quot;) %&gt;% pull(initial) %&gt;% .[[1]] init_design_h0@c2_pivots &lt;- rep(2, order) ub_design &lt;- TwoStageDesign( 3 * init_design_h0@n1, 2, 3, rep(600, order), rep(3.0, order) ) 3.3.4 Optimization opt_neg &lt;- minimize( ess_0, subject_to( toer_cnstr, epow_cnstr ), initial_design = init_design_h0, upper_boundary_design = ub_design, opts = opts ) 3.3.5 Test Cases First of all, check if the optimization algorithm converged. To avoid improper solutions, it is first verified that the maximum number of iterations was not exceeded in any of the three cases. testthat::expect_true(opt_neg$nloptr_return$iterations &lt; opts$maxeval) print(opt_neg$nloptr_return$iterations) ## [1] 7197 Again, the type one error rate under the point null hypothesis \\(\\delta = 0\\) can be tested by simulation. tbl_toer &lt;- tibble( toer = evaluate(Power(datadist, H_0), opt_neg$design), toer_sim = sim_pr_reject(opt_neg$design, .0, datadist)$prob ) print(tbl_toer) ## # A tibble: 1 × 2 ## toer toer_sim ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.0249 0.0250 testthat::expect_true(tbl_toer$toer &lt;= alpha * (1 + tol)) testthat::expect_true(tbl_toer$toer_sim &lt;= alpha * (1 + tol)) Furthermore, the expected sample size under the prior conditioned on negative effect sizes (\\(\\delta \\leq 0\\)) should be lower for the optimal design derived in this variant than for the optimal design from Variant II.1 where expected sample size under the full prior was minimized. testthat::expect_lte( evaluate(ess_0, opt_neg$design), evaluate( ess_0, tbl_designs %&gt;% filter(type == &quot;two-stage&quot;) %&gt;% pull(optimal) %&gt;% .[[1]] %&gt;% .$design ) ) 3.4 Variant II-3: Conditional Power Constraint 3.4.1 Objective As in Variant II-1, expected sample size under the full prior is minimized. 3.4.2 Constraints In addition to the constraints on type one error rate and expected power, a constraint on conditional power to be larger than \\(0.7\\) is included. cp &lt;- ConditionalPower(datadist, condition(prior, c(0, prior@support[2]))) cp_cnstr &lt;- cp &gt;= 0.7 3.4.3 Initial Design The previous initial design can still be applied. 3.4.4 Optimization opt_cp &lt;- minimize( ess, subject_to( toer_cnstr, epow_cnstr, cp_cnstr ), initial_design = tbl_designs$initial[[3]], opts = opts ) 3.4.5 Test Cases We start checking whether the maximum number of iterations was not reached. print(opt_cp$nloptr_return$iterations) ## [1] 2012 testthat::expect_true(opt_cp$nloptr_return$iterations &lt; opts$maxeval) The type one error rate is tested via simulation and compared to the value obtained by evaluate(). tbl_toer &lt;- tibble( toer = evaluate(Power(datadist, H_0), opt_cp$design), toer_sim = sim_pr_reject(opt_cp$design, .0, datadist)$prob ) print(tbl_toer) ## # A tibble: 1 × 2 ## toer toer_sim ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.0250 0.0250 testthat::expect_true(tbl_toer$toer &lt;= alpha * (1 + tol)) testthat::expect_true(tbl_toer$toer_sim &lt;= alpha * (1 + tol)) The conditional power is evaluated via numerical integration on several points inside the continuation region and it is tested whether the constraint is fulfilled on all these points. tibble( x1 = seq(opt_cp$design@c1f, opt_cp$design@c1e, length.out = 25), cp = map_dbl(x1, ~evaluate(cp, opt_cp$design, .)) ) %&gt;% {print(.); .} %&gt;% { testthat::expect_true(all(.$cp &gt;= 0.7 * (1 - tol))) } ## # A tibble: 25 × 2 ## x1 cp ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.675 0.698 ## 2 0.747 0.700 ## 3 0.819 0.700 ## 4 0.891 0.699 ## 5 0.963 0.698 ## 6 1.03 0.698 ## 7 1.11 0.701 ## 8 1.18 0.705 ## 9 1.25 0.709 ## 10 1.32 0.715 ## # … with 15 more rows Due to the additional constraint in comparison to Variant II.1, Variant II.3 should show a larger expected sample size under the prior than Variant II.1 testthat::expect_gte( evaluate(ess, opt_cp$design), evaluate( ess, tbl_designs %&gt;% filter(type == &quot;two-stage&quot;) %&gt;% pull(optimal) %&gt;% .[[1]] %&gt;% .$design ) ) 3.5 Plot Two-Stage Designs The optimal two-stage designs stemming from the different variants are plotted together. "],["scenarioIII.html", "4 Scenario III: large effect, uniform prior 4.1 Details 4.2 Variant III.1: Convergence under prior concentration", " 4 Scenario III: large effect, uniform prior 4.1 Details This scenario covers a similar setting as Scenario I. The purpose is to asses whether placing uniform priors with decreasing width of support centered at \\(\\delta=0.4\\) leads to a sequence of optimal designs which converges towards the solution in variant I-1. The trial is still considered to be two-armed with normally distribtuted outcomes and the type one error rate under the null hypothesis \\(\\mathcal{H}_0:\\delta \\leq 0\\) is to be protected at \\(\\alpha = 0.025\\). datadist &lt;- Normal(two_armed = TRUE) H_0 &lt;- PointMassPrior(.0, 1) alpha &lt;- 0.025 toer_cnstr &lt;- Power(datadist, H_0) &lt;= alpha In this scenario we consider a sequence of uniform distributions \\(\\delta\\sim\\operatorname{Unif}(0.4 - \\Delta_i, 0.4 + \\Delta_i)\\) around \\(0.4\\) with \\(\\Delta_i=(3 - i)/10\\) for \\(i=0\\ldots 3\\). I.e., for \\(\\Delta_3=0\\) reduces to PointMassPrior on \\(\\delta=0.4\\). prior &lt;- function(delta) { if (delta == 0) return(PointMassPrior(.4, 1.0)) a &lt;- .4 - delta; b &lt;- .4 + delta ContinuousPrior(function(x) dunif(x, a, b), support = c(a, b)) } Across all variants in this scenario, the expected power under the respective prior conditioned on \\(\\delta &gt; 0\\) must be at least \\(0.8\\). I.e., throughout this scenario, we always use the following constraint on expected power. ep_cnstr &lt;- function(delta) { prior &lt;- prior(delta) cnd_prior &lt;- condition(prior, c(0, bounds(prior)[2])) return( Power(datadist, cnd_prior) &gt;= 0.8 ) } 4.2 Variant III.1: Convergence under prior concentration The goal of this variant is to make sure that the optimal solution converges as the prior is more and more concentrated at a point mass. 4.2.1 Objective Expected sample size under the respective prior is minimized, i.e., \\(\\boldsymbol{E}\\big[n(\\mathcal{D})\\big]\\). objective &lt;- function(delta) { ExpectedSampleSize(datadist, prior(delta)) } 4.2.2 Constraints The constraints have already been described under details. 4.2.3 Optimization problem The optimization problem depending on \\(\\Delta_i\\) is defined below. The default optimization parameters, 5 pivot points, and a fixed initial design are used. The initial design is chosen such that the error constraints are fulfilled. Early stopping for futility is applied if the effect shows in the opponent direction to the alternative, i.e. \\(c_1^f=0\\). \\(c_2\\) is chosen close to and \\(c_1^e\\) a little larger than the \\(1-\\alpha\\)-quantile of the standard normal distribution. The sample sizes are selected to fulfill the error constraints. init &lt;- TwoStageDesign( n1 = 150, c1f = 0, c1e = 2.3, n2 = 125.0, c2 = 2.0, order = 5 ) optimal_design &lt;- function(delta) { minimize( objective(delta), subject_to( toer_cnstr, ep_cnstr(delta) ), initial_design = init ) } # compute the sequence of optimal designs deltas &lt;- 3:0/10 results &lt;- lapply(deltas, optimal_design) 4.2.4 Test cases Check that iteration limit was not exceeded in any case. iters &lt;- sapply(results, function(x) x$nloptr_return$iterations) print(iters) ## [1] 2368 1891 2685 2789 testthat::expect_true(all(iters &lt;= opts$maxeval)) Check type one error rate control by simulation and by calling evaluate(). df_toer &lt;- tibble( delta = deltas, toer = sapply(results, function(x) evaluate(Power(datadist, H_0), x$design)), toer_sim = sapply(results, function(x) sim_pr_reject(x$design, .0, datadist)$prob) ) testthat::expect_true(all(df_toer$toer &lt;= alpha * (1 + tol))) testthat::expect_true(all(df_toer$toer_sim &lt;= alpha * (1 + tol))) print(df_toer) ## # A tibble: 4 × 3 ## delta toer toer_sim ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.3 0.0250 0.0250 ## 2 0.2 0.0250 0.0250 ## 3 0.1 0.0250 0.0250 ## 4 0 0.0250 0.0250 Check that expected sample size decreases with decreasing prior variance. testthat::expect_gte( evaluate(objective(deltas[1]), results[[1]]$design), evaluate(objective(deltas[2]), results[[2]]$design) ) testthat::expect_gte( evaluate(objective(deltas[2]), results[[2]]$design), evaluate(objective(deltas[3]), results[[3]]$design) ) testthat::expect_gte( evaluate(objective(deltas[3]), results[[3]]$design), evaluate(objective(deltas[4]), results[[4]]$design) ) 4.2.5 Plot designs Finally, we plot the designs and assess for convergence. "],["scenarioIV.html", "5 Scenario IV: smaller effect, point prior 5.1 Details 5.2 Variant IV-1: Minimizing Expected Sample Size under Point Prior 5.3 Variant IV-2: Increase Power 5.4 Variant IV-3: Increase Type One Error rate 5.5 Plot Two-Stage Designs", " 5 Scenario IV: smaller effect, point prior 5.1 Details In this scenario, we return to point priors as investigated in Scenario I. The main goal is to validate adoptr’s sensitivity with regard to the assumed effect size and the constraints on power and type one error rate. Therefore, we still assume a two-armed trial with normally distributed outcomes. The assumed effect size under the alternative is \\(\\delta = 0.2\\) in this setting. Type one error rate is protected at \\(2.5\\%\\) and the power should be at least \\(80\\%\\). We will vary these values in the variants IV.2 and IV.3 # data distribution and hypotheses datadist &lt;- Normal(two_armed = TRUE) H_0 &lt;- PointMassPrior(.0, 1) prior &lt;- PointMassPrior(.2, 1) # constraints alpha &lt;- 0.025 min_power &lt;- 0.8 toer_cnstr &lt;- Power(datadist, H_0) &lt;= alpha pow_cnstr &lt;- Power(datadist, prior) &gt;= min_power 5.2 Variant IV-1: Minimizing Expected Sample Size under Point Prior 5.2.1 Objective Expected sample size under the alternative point prior \\(\\delta = 0.2\\) is minimized. ess &lt;- ExpectedSampleSize(datadist, prior) 5.2.2 Constraints No additional constraints are considered in this variant. 5.2.3 Initial Design For this example, the optimal one-stage, group-sequential, and generic two-stage designs are computed. The initial design that is used as starting value of optimization is defined as a group-sequential design by the package rpact that fulfills type one error rate and power constraints in the case of group-sequential and two-stage design. The initial one-stage design is chosen heuristically. The order of integration is set to \\(5\\). order &lt;- 5L tbl_designs &lt;- tibble( type = c(&quot;one-stage&quot;, &quot;group-sequential&quot;, &quot;two-stage&quot;), initial = list( OneStageDesign(500, 2.0), rpact_design(datadist, 0.2, 0.025, 0.8, TRUE, order), TwoStageDesign(rpact_design(datadist, 0.2, 0.025, 0.8, TRUE, order))) ) 5.2.4 Optimization tbl_designs &lt;- tbl_designs %&gt;% mutate( optimal = purrr::map(initial, ~minimize( ess, subject_to( toer_cnstr, pow_cnstr ), initial_design = ., opts = opts)) ) 5.2.5 Test Cases Firstly, it is checked whether the maximum number of iterations was not exceeded in all three cases. tbl_designs %&gt;% transmute( type, iterations = purrr::map_int(tbl_designs$optimal, ~.$nloptr_return$iterations) ) %&gt;% {print(.); .} %&gt;% {testthat::expect_true(all(.$iterations &lt; opts$maxeval))} ## # A tibble: 3 × 2 ## type iterations ## &lt;chr&gt; &lt;int&gt; ## 1 one-stage 20 ## 2 group-sequential 959 ## 3 two-stage 2096 Now, the constraints on type one error rate and power are tested via simulation. tbl_designs %&gt;% transmute( type, toer = purrr::map(tbl_designs$optimal, ~sim_pr_reject(.[[1]], .0, datadist)$prob), power = purrr::map(tbl_designs$optimal, ~sim_pr_reject(.[[1]], .2, datadist)$prob) ) %&gt;% unnest(., cols = c(toer, power)) %&gt;% {print(.); .} %&gt;% { testthat::expect_true(all(.$toer &lt;= alpha * (1 + tol))) testthat::expect_true(all(.$power &gt;= min_power * (1 - tol))) } ## # A tibble: 3 × 3 ## type toer power ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 one-stage 0.0251 0.799 ## 2 group-sequential 0.0250 0.800 ## 3 two-stage 0.0250 0.800 Due to increasing degrees of freedom, the expected sample sizes under the alternative should be ordered as ‘one-stage &gt; group-sequential &gt; two-stage’. They are evaluated by simulation as well as by evaluate(). tbl_designs %&gt;% mutate( ess = map_dbl(optimal, ~evaluate(ess, .$design) ), ess_sim = map_dbl(optimal, ~sim_n(.$design, .2, datadist)$n ) ) %&gt;% {print(.); .} %&gt;% { # sim/evaluate same under alternative? testthat::expect_equal(.$ess, .$ess_sim, tolerance = tol_n, scale = 1) # monotonicity with respect to degrees of freedom testthat::expect_true(all(diff(.$ess) &lt; 0)) } ## # A tibble: 3 × 5 ## type initial optimal ess ess_sim ## &lt;chr&gt; &lt;list&gt; &lt;list&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 one-stage &lt;OnStgDsg&gt; &lt;adptrOpR [3]&gt; 392 392 ## 2 group-sequential &lt;GrpSqntD&gt; &lt;adptrOpR [3]&gt; 324. 323. ## 3 two-stage &lt;TwStgDsg&gt; &lt;adptrOpR [3]&gt; 320. 319. Furthermore, the expected sample size under the alternative of the optimal group-sequential design should be lower than for the group-sequential design by rpact that is based on the inverse normal combination test. tbl_designs %&gt;% filter(type == &quot;group-sequential&quot;) %&gt;% { expect_lte( evaluate(ess, {.[[&quot;optimal&quot;]][[1]]$design}), evaluate(ess, {.[[&quot;initial&quot;]][[1]]}) ) } Finally, the \\(n_2\\) function of the optimal two-stage design is expected to be monotonously decreasing: expect_true( all(diff( # get optimal two-stage design n2 pivots tbl_designs %&gt;% filter(type == &quot;two-stage&quot;) %&gt;% {.[[&quot;optimal&quot;]][[1]]$design@n2_pivots} ) &lt; 0) ) 5.3 Variant IV-2: Increase Power 5.3.1 Objective The objective remains expected sample size under the alternative \\(\\delta = 0.2\\). 5.3.2 Constraints The minimal required power is increased to \\(90\\%\\). min_power_2 &lt;- 0.9 pow_cnstr_2 &lt;- Power(datadist, prior) &gt;= min_power_2 5.3.3 Initial Design For both flavours with two stages (group-sequential, generic two-stage) the initial design is created by rpact to fulfill the error rate constraints. tbl_designs_9 &lt;- tibble( type = c(&quot;one-stage&quot;, &quot;group-sequential&quot;, &quot;two-stage&quot;), initial = list( OneStageDesign(500, 2.0), rpact_design(datadist, 0.2, 0.025, 0.9, TRUE, order), TwoStageDesign(rpact_design(datadist, 0.2, 0.025, 0.9, TRUE, order))) ) 5.3.4 Optimization tbl_designs_9 &lt;- tbl_designs_9 %&gt;% mutate( optimal = purrr::map(initial, ~minimize( ess, subject_to( toer_cnstr, pow_cnstr_2 ), initial_design = ., opts = opts)) ) 5.3.5 Test Cases We start checking if the maximum number of iterations was not exceeded in all three cases. tbl_designs_9 %&gt;% transmute( type, iterations = purrr::map_int(tbl_designs_9$optimal, ~.$nloptr_return$iterations) ) %&gt;% {print(.); .} %&gt;% {testthat::expect_true(all(.$iterations &lt; opts$maxeval))} ## # A tibble: 3 × 2 ## type iterations ## &lt;chr&gt; &lt;int&gt; ## 1 one-stage 30 ## 2 group-sequential 1485 ## 3 two-stage 3069 The type one error rate and power constraints are evaluated by simulation. tbl_designs_9 %&gt;% transmute( type, toer = purrr::map(tbl_designs_9$optimal, ~sim_pr_reject(.[[1]], .0, datadist)$prob), power = purrr::map(tbl_designs_9$optimal, ~sim_pr_reject(.[[1]], .2, datadist)$prob) ) %&gt;% unnest(., cols = c(toer, power)) %&gt;% {print(.); .} %&gt;% { testthat::expect_true(all(.$toer &lt;= alpha * (1 + tol))) testthat::expect_true(all(.$power &gt;= min_power_2 * (1 - tol))) } ## # A tibble: 3 × 3 ## type toer power ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 one-stage 0.0251 0.900 ## 2 group-sequential 0.0249 0.900 ## 3 two-stage 0.0250 0.900 Due to increasing degrees of freedom, the expected sample sizes under the alternative should be ordered as ‘one-stage &gt; group-sequential &gt; two-stage’. This is tested by simulation as well as by evaluate(). tbl_designs_9 %&gt;% mutate( ess = map_dbl(optimal, ~evaluate(ess, .$design) ), ess_sim = map_dbl(optimal, ~sim_n(.$design, .2, datadist)$n ) ) %&gt;% {print(.); .} %&gt;% { # sim/evaluate same under alternative? testthat::expect_equal(.$ess, .$ess_sim, tolerance = tol_n, scale = 1) # monotonicity with respect to degrees of freedom testthat::expect_true(all(diff(.$ess) &lt; 0)) testthat::expect_true(all(diff(.$ess_sim) &lt; 0))} ## # A tibble: 3 × 5 ## type initial optimal ess ess_sim ## &lt;chr&gt; &lt;list&gt; &lt;list&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 one-stage &lt;OnStgDsg&gt; &lt;adptrOpR [3]&gt; 525 525 ## 2 group-sequential &lt;GrpSqntD&gt; &lt;adptrOpR [3]&gt; 405. 405. ## 3 two-stage &lt;TwStgDsg&gt; &lt;adptrOpR [3]&gt; 397. 397. Comparing with the inverse-normal based group-sequential design created by rpact, the optimal group-sequential design should show a lower expected sample size under the point alternative. tbl_designs_9 %&gt;% filter(type == &quot;group-sequential&quot;) %&gt;% { expect_lte( evaluate(ess, {.[[&quot;optimal&quot;]][[1]]$design}), evaluate(ess, {.[[&quot;initial&quot;]][[1]]}) ) } Since a point prior is regarded, the \\(n_2\\) function of the optimal two-stage design is expected to be monotonously decreasing: expect_true( all(diff( # get optimal two-stage design n2 pivots tbl_designs_9 %&gt;% filter(type == &quot;two-stage&quot;) %&gt;% {.[[&quot;optimal&quot;]][[1]]$design@n2_pivots} ) &lt; 0) ) 5.4 Variant IV-3: Increase Type One Error rate 5.4.1 Objective As in variants IV.1 and IV-2, expected sample size under the point alternative is minimized. 5.4.2 Constraints While the power is still lower bounded by \\(90\\%\\) as in variant II, the maximal type one error rate is increased to \\(5\\%\\). alpha_2 &lt;- .05 toer_cnstr_2 &lt;- Power(datadist, H_0) &lt;= alpha_2 5.4.3 Initial Design Again, a design computed by means of the package rpact to fulfill the updated error rate constraints is applied as initial design for the optimal group-sequential and generic two-stage designs. tbl_designs_5 &lt;- tibble( type = c(&quot;one-stage&quot;, &quot;group-sequential&quot;, &quot;two-stage&quot;), initial = list( OneStageDesign(500, 2.0), rpact_design(datadist, 0.2, 0.05, 0.9, TRUE, order), TwoStageDesign(rpact_design(datadist, 0.2, 0.05, 0.9, TRUE, order))) ) 5.4.4 Optimization tbl_designs_5 &lt;- tbl_designs_5 %&gt;% mutate( optimal = purrr::map(initial, ~minimize( ess, subject_to( toer_cnstr_2, pow_cnstr_2 ), initial_design = ., opts = opts)) ) 5.4.5 Test Cases The convergence of the optimization algorithm is tested by checking if the maximum number of iterations was not exceeded. tbl_designs_5 %&gt;% transmute( type, iterations = purrr::map_int(tbl_designs_5$optimal, ~.$nloptr_return$iterations) ) %&gt;% {print(.); .} %&gt;% {testthat::expect_true(all(.$iterations &lt; opts$maxeval))} ## # A tibble: 3 × 2 ## type iterations ## &lt;chr&gt; &lt;int&gt; ## 1 one-stage 27 ## 2 group-sequential 1127 ## 3 two-stage 2338 By simulation, the constraints on the error rates (type one error and power) are tested. tbl_designs_5 %&gt;% transmute( type, toer = purrr::map(tbl_designs_5$optimal, ~sim_pr_reject(.[[1]], .0, datadist)$prob), power = purrr::map(tbl_designs_5$optimal, ~sim_pr_reject(.[[1]], .2, datadist)$prob) ) %&gt;% unnest(., cols = c(toer, power)) %&gt;% {print(.); .} %&gt;% { testthat::expect_true(all(.$toer &lt;= alpha_2 * (1 + tol))) testthat::expect_true(all(.$power &gt;= min_power_2 * (1 - tol))) } ## # A tibble: 3 × 3 ## type toer power ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 one-stage 0.0502 0.900 ## 2 group-sequential 0.0500 0.900 ## 3 two-stage 0.0502 0.900 Due to increasing degrees of freedom, the expected sample sizes under the alternative should be ordered as ‘one-stage &gt; group-sequential &gt; two-stage’. They are tested by simulation as well as by calling evaluate(). tbl_designs_5 %&gt;% mutate( ess = map_dbl(optimal, ~evaluate(ess, .$design) ), ess_sim = map_dbl(optimal, ~sim_n(.$design, .2, datadist)$n ) ) %&gt;% {print(.); .} %&gt;% { # sim/evaluate same under alternative? testthat::expect_equal(.$ess, .$ess_sim, tolerance = tol_n, scale = 1) # monotonicity with respect to degrees of freedom testthat::expect_true(all(diff(.$ess) &lt; 0)) } ## # A tibble: 3 × 5 ## type initial optimal ess ess_sim ## &lt;chr&gt; &lt;list&gt; &lt;list&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 one-stage &lt;OnStgDsg&gt; &lt;adptrOpR [3]&gt; 428 428 ## 2 group-sequential &lt;GrpSqntD&gt; &lt;adptrOpR [3]&gt; 325. 325. ## 3 two-stage &lt;TwStgDsg&gt; &lt;adptrOpR [3]&gt; 319. 319. The expected sample size under the alternative that was used as objective criterion of the optimal group-sequential design should be lower than for the group-sequential design by rpact that is based on the inverse normal combination test. tbl_designs_5 %&gt;% filter(type == &quot;group-sequential&quot;) %&gt;% { expect_lte( evaluate(ess, {.[[&quot;optimal&quot;]][[1]]$design}), evaluate(ess, {.[[&quot;initial&quot;]][[1]]}) ) } Also in this variant, the \\(n_2\\) function of the optimal two-stage design is expected to be monotonously decreasing: expect_true( all(diff( # get optimal two-stage design n2 pivots tbl_designs_5 %&gt;% filter(type == &quot;two-stage&quot;) %&gt;% {.[[&quot;optimal&quot;]][[1]]$design@n2_pivots} ) &lt; 0) ) 5.5 Plot Two-Stage Designs The optimal two-stage designs stemming from the three different variants are plotted together. "],["scenarioV.html", "6 Scenario V: single-arm design, medium effect size 6.1 Details 6.2 Variant V-1, sensitivity to integration order 6.3 Variant V-2, utility maximization 6.4 Variant V-3, n1-penalty 6.5 Variant V-4, n2-penalty", " 6 Scenario V: single-arm design, medium effect size 6.1 Details In this scenario, again a point prior is analyzed. The null hypothesis is \\(\\delta \\leq 0\\) and we assume an alternative effect size of \\(\\delta = 0.3\\). Type one error rate should be protected at 2.5% and the design’s power should be at least 80%. Differently than in the previous scenarios, we are assuming a single-arm design throughout this scenario. # data distribution and hypotheses datadist &lt;- Normal(two_armed = FALSE) H_0 &lt;- PointMassPrior(.0, 1) prior &lt;- PointMassPrior(.3, 1) # define constraints alpha &lt;- 0.025 min_power &lt;- 0.8 toer_cnstr &lt;- Power(datadist, H_0) &lt;= alpha pow_cnstr &lt;- Power(datadist, prior) &gt;= min_power 6.2 Variant V-1, sensitivity to integration order In this variant, the sensitivity of the optimization with respect to the integration order is investigated. We apply three different integration orders: 5, 8, and 11. 6.2.1 Objective Expected sample size under the alternative point mass prior \\(\\delta = 0.3\\) is minimized. ess &lt;- ExpectedSampleSize(datadist, prior) 6.2.2 Constraints No additional constraints are considered in this variant. 6.2.3 Initial Design In order to vary the initial design, rpact is not used in this variant. Instead, the following heuristical considerations are made. A fixed design for these parameters would require 176 subjects per group. We use the half of this as initial values for the sample sizes. The initial stop for futility is at \\(c_1^f=0\\), i.e., if the effect shows in the opponent direction to the alternative. The starting values for the efficacy stop and for \\(c_2\\) is the \\(1-\\alpha\\)- quantile of the normal distribution. init_design &lt;- function(order) { TwoStageDesign( n1 = ceiling(pwr::pwr.t.test(d = .3, sig.level = .025, power = .8, alternative = &quot;greater&quot;)$n) / 2, c1f = 0, c1e = qnorm( 1 - 0.025), n2 = ceiling(pwr::pwr.t.test(d = .3, sig.level = .025, power = .8, alternative = &quot;greater&quot;)$n) / 2, c2 = qnorm(1 - 0.025), order = order ) } 6.2.4 Optimization The optimal design is computed for three different integration orders: 5, 8, and 11. opt_design &lt;- function(order) { minimize( ess, subject_to( toer_cnstr, pow_cnstr ), initial_design = init_design(order), upper_boundary_design = TwoStageDesign(200, 2, 4, 200, 3, order), opts = opts ) } opt &lt;- tibble( order = c(5, 8, 11), design = lapply(c(5, 8, 11), function(x) opt_design(x)) ) 6.2.5 Test cases Check if the optimization algorithm converged in all cases. opt %&gt;% transmute( order, iterations = purrr::map_int(opt$design, ~.$nloptr_return$iterations) ) %&gt;% {print(.); .} %&gt;% {testthat::expect_true(all(.$iterations &lt; opts$maxeval))} ## # A tibble: 3 × 2 ## order iterations ## &lt;dbl&gt; &lt;int&gt; ## 1 5 2372 ## 2 8 5378 ## 3 11 10689 Test the constraints on type one error rate and power by simulation and compare the results to the outcome of evaluate(). opt %&gt;% transmute( order, toer = map_dbl(design, ~evaluate(Power(datadist, PointMassPrior(.0, 1)), .$design) ), toer_sim = map_dbl(opt$design, ~sim_pr_reject(.[[1]], .0, datadist)$prob), power = map_dbl(design, ~evaluate(Power(datadist, PointMassPrior(.3, 1)), .$design) ), power_sim = map_dbl(opt$design, ~sim_pr_reject(.[[1]], .3, datadist)$prob), ess = map_dbl(design, ~evaluate(ess, .$design) ), ess_sim = map_dbl(opt$design, ~sim_n(.[[1]], .3, datadist)$n) ) %&gt;% unnest(., cols = c(toer, toer_sim, power, power_sim)) %&gt;% {print(.); .} %&gt;% { testthat::expect_true(all(.$toer &lt;= alpha * (1 + tol))) testthat::expect_true(all(.$toer_sim &lt;= alpha * (1 + tol))) testthat::expect_true(all(.$power &gt;= min_power * (1 - tol))) testthat::expect_true(all(.$power_sim &gt;= min_power * (1 - tol))) } ## # A tibble: 3 × 7 ## order toer toer_sim power power_sim ess ess_sim ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 5 0.0250 0.0250 0.799 0.800 71.0 71.0 ## 2 8 0.0250 0.0250 0.800 0.800 71.0 71.0 ## 3 11 0.0250 0.0250 0.800 0.800 71.0 71.0 6.3 Variant V-2, utility maximization 6.3.1 Objective In this variant, a utility function consisting of expected sample size and power is minimized. The parameter \\(\\lambda\\) that is describing the ratio between expected sample size and power is varied. pow &lt;- Power(datadist, prior) ess &lt;- ExpectedSampleSize(datadist, prior) obj &lt;- function(lambda) { composite({ess - lambda * pow}) } 6.3.2 Constraints The type one error rate is controlled at 0.025 on the boundary of the null hypothesis. Hence, the previous inequality can still be used. There is no constraint on power any more because power is part of the objective utility function. 6.3.3 Initial Design The previous initial design with order \\(5\\) is applied. 6.3.4 Optimization The optimal design is computed for two values of \\(\\lambda\\): 100 and 200. opt_utility &lt;- tibble( lambda = c(100, 200) ) %&gt;% mutate( design = purrr::map(lambda, ~minimize( obj(.), subject_to( toer_cnstr ), initial_design = init_design(5), opts = opts)) ) 6.3.5 Test cases Firstly, it is checked whether the maximum number of iterations was not exceeded in both flavours. opt_utility %&gt;% transmute( lambda, iterations = purrr::map_int(opt_utility$design, ~.$nloptr_return$iterations) ) %&gt;% {print(.); .} %&gt;% {testthat::expect_true(all(.$iterations &lt; opts$maxeval))} ## # A tibble: 2 × 2 ## lambda iterations ## &lt;dbl&gt; &lt;int&gt; ## 1 100 3338 ## 2 200 3154 Type one error rate control is tested for both designs by simulation and by adoptr’s function evaluate. In addition, it is tested if the design with larger \\(\\lambda\\) (i.e., stronger focus on power), shows the larger overall power. opt_utility %&gt;% transmute( lambda, toer = map_dbl(design, ~evaluate(Power(datadist, PointMassPrior(.0, 1)), .$design) ), toer_sim = purrr::map(opt_utility$design, ~sim_pr_reject(.[[1]], .0, datadist)$prob), power = map_dbl(design, ~evaluate(Power(datadist, PointMassPrior(.3, 1)), .$design) ), power_sim = purrr::map(opt_utility$design, ~sim_pr_reject(.[[1]], .3, datadist)$prob) ) %&gt;% unnest(., c(toer, toer_sim, power, power_sim)) %&gt;% {print(.); .} %&gt;% { testthat::expect_true(all(.$toer &lt;= alpha * (1 + tol))) testthat::expect_true(all(.$toer_sim &lt;= alpha * (1 + tol))) testthat::expect_lte(.$power[1], .$power[2]) } ## # A tibble: 2 × 5 ## lambda toer toer_sim power power_sim ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 100 0.0250 0.0250 0.519 0.520 ## 2 200 0.0251 0.0250 0.897 0.897 Finally, the three designs computed so far are plotted together to allow comparison. 6.4 Variant V-3, n1-penalty In this variant, the influence of the regularization term N1() is investigated. 6.4.1 Objective In order to analyse the influence of N1(), a mixed criterion consisting of expected sample size under the point prior and \\(N1()\\) is minimized. N1 &lt;- N1() obj_n1 &lt;- function(lambda) { composite({ess + lambda * N1}) } 6.4.2 Constraints The inequalities from variant V.1 can still be used. 6.4.3 Initial Design The previous initial design with order \\(5\\) is applied. This variant requires an upper bound on \\(c_2\\). Otherwise, very large values for \\(c_2\\) and large \\(n_2\\)-values would allow appear to reduce \\(n_1\\). ub_design &lt;- get_upper_boundary_design(init_design(5)) ub_design@c2_pivots &lt;- rep(3, 5) 6.4.4 Optimization The optimal design is computed for two values of \\(\\lambda\\): 0.05 and 0.2. opt_n1 &lt;- tibble( lambda = c(0.05, 0.2) ) %&gt;% mutate( design = purrr::map(lambda, ~minimize( obj_n1(.), subject_to( toer_cnstr, pow_cnstr ), initial_design = init_design(5), upper_boundary_design = ub_design, opts = opts)) ) 6.4.5 Test cases We start testing if the optimization algorithm converged in both cases opt_n1 %&gt;% transmute( lambda, iterations = purrr::map_int(opt_n1$design, ~.$nloptr_return$iterations) ) %&gt;% {print(.); .} %&gt;% {testthat::expect_true(all(.$iterations &lt; opts$maxeval))} ## # A tibble: 2 × 2 ## lambda iterations ## &lt;dbl&gt; &lt;int&gt; ## 1 0.05 3133 ## 2 0.2 2921 Next, the error rate constraints on type one error rate and power are both tested by simulation and by the evaluate-call. opt_n1 %&gt;% transmute( lambda, toer = map_dbl(design, ~evaluate(Power(datadist, PointMassPrior(.0, 1)), .$design) ), toer_sim = purrr::map(opt_n1$design, ~sim_pr_reject(.[[1]], .0, datadist)$prob), power = map_dbl(design, ~evaluate(Power(datadist, PointMassPrior(.3, 1)), .$design) ), power_sim = purrr::map(opt_n1$design, ~sim_pr_reject(.[[1]], .3, datadist)$prob) ) %&gt;% unnest(., cols = c(toer, toer_sim, power, power_sim)) %&gt;% {print(.); .} %&gt;% { testthat::expect_true(all(.$toer &lt;= alpha * (1 + tol))) testthat::expect_true(all(.$toer_sim &lt;= alpha * (1 + tol))) testthat::expect_true(all(.$power &gt;= min_power * (1 - tol))) testthat::expect_true(all(.$power_sim &gt;= min_power * (1 - tol))) } ## # A tibble: 2 × 5 ## lambda toer toer_sim power power_sim ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.05 0.0250 0.0250 0.801 0.801 ## 2 0.2 0.0250 0.0250 0.800 0.800 Since \\(n_1\\) is penalized in both flavours that are computed in this variant, we expect a lower \\(n_1\\) value as larger \\(\\lambda\\). Furthermore, \\(n_1\\) should be lower in both cases than in the unpenalized situation regarded in variant V.1. Finally, these three designs are plotted together to allow graphical comparison. ## # A tibble: 3 × 3 ## type design n_1 ## &lt;chr&gt; &lt;list&gt; &lt;dbl&gt; ## 1 No Penalization on n_1 &lt;TwStgDsg&gt; 50 ## 2 Penalization on n_1 with lambda = 0.05 &lt;TwStgDsg&gt; 48 ## 3 Penalization on n_1 with lambda = 0.2 &lt;TwStgDsg&gt; 41 6.5 Variant V-4, n2-penalty Here, we alter the situation from variant V.3 by not penalizing \\(n_1\\), but the average stage-two sample size \\(n_2\\). This can be done by means of the function AverageN2(). 6.5.1 Objective As in variant V.3, a mixed criterion is minimized. Here, it consists of expected sample size under the point prior and the average of \\(n_2\\). avn2 &lt;- AverageN2() obj_n2 &lt;- function(lambda) { composite({ess + lambda * avn2}) } 6.5.2 Constraints The inequalities from variant V.1 can still be used. 6.5.3 Initial Design The previous initial design with order \\(8\\) is applied. However, this case requires the definition of an upper-bound for \\(c_2\\). Otherwise, very small \\(n_2\\)-values and very large \\(c_2\\)-values would appear close to the early-futility-stop boundary in order to decrease the average \\(n_2\\). ub_design &lt;- get_upper_boundary_design(init_design(8)) ub_design@c2_pivots &lt;- rep(2.5, 8) 6.5.4 Optimization The optimal design is computed for two values of \\(\\lambda\\): 0.01 and 0.1. opt_n2 &lt;- tibble( lambda = c(0.01, 0.1) ) %&gt;% mutate( design = purrr::map(lambda, ~minimize( obj_n2(.), subject_to( toer_cnstr, pow_cnstr ), initial_design = init_design(8), upper_boundary_design = ub_design, opts = opts)) ) 6.5.5 Test cases As first step, we check if the maximum number of iterations was not exceeded in both cases. opt_n2 %&gt;% transmute( lambda, iterations = purrr::map_int(opt_n2$design, ~.$nloptr_return$iterations) ) %&gt;% {print(.); .} %&gt;% {testthat::expect_true(all(.$iterations &lt; opts$maxeval))} ## # A tibble: 2 × 2 ## lambda iterations ## &lt;dbl&gt; &lt;int&gt; ## 1 0.01 18398 ## 2 0.1 11123 As second step, the type one error rate and power restrictions are tested by simulation and by calling evaluate. opt_n2 %&gt;% transmute( lambda, toer = map_dbl(design, ~evaluate(Power(datadist, PointMassPrior(.0, 1)), .$design) ), toer_sim = purrr::map(opt_n2$design, ~sim_pr_reject(.[[1]], .0, datadist)$prob), power = map_dbl(design, ~evaluate(Power(datadist, PointMassPrior(.3, 1)), .$design) ), power_sim = purrr::map(opt_n2$design, ~sim_pr_reject(.[[1]], .3, datadist)$prob) ) %&gt;% unnest(., c(toer, toer_sim, power, power_sim)) %&gt;% {print(.); .} %&gt;% { testthat::expect_true(all(.$toer &lt;= alpha * (1 + tol))) testthat::expect_true(all(.$toer_sim &lt;= alpha * (1 + tol))) testthat::expect_true(all(.$power &gt;= min_power * (1 - tol))) testthat::expect_true(all(.$power_sim &gt;= min_power * (1 - tol))) } ## # A tibble: 2 × 5 ## lambda toer toer_sim power power_sim ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.01 0.0250 0.0249 0.802 0.802 ## 2 0.1 0.0250 0.0250 0.801 0.801 Due to increasing penalization, it is assumed that the optimal design computed in variant V.1 (no penalization) shows a larger average \\(n_2\\) than the optimal penalized design with \\(\\lambda = 0.01\\) and this shows a larger average \\(n_2\\) than the optimal design with \\(\\lambda = 0.1\\). Additionally, these three designs are depicted in a joint plot. ## # A tibble: 3 × 3 ## type design average_n2 ## &lt;chr&gt; &lt;list&gt; &lt;dbl&gt; ## 1 No Penalization on AvN2 &lt;TwStgDsg&gt; 47.6 ## 2 Penalization on AvN2 with lambda = 0.01 &lt;TwStgDsg&gt; 46.6 ## 3 Penalization on AvN2 with lambda = 0.1 &lt;TwStgDsg&gt; 38.9 "],["scenarioVI.html", "7 Scenario VI: binomial distribution 7.1 Details 7.2 Variant VI-1, efficiency of optimal designs for binomial distribution", " 7 Scenario VI: binomial distribution 7.1 Details In this scenario, the implemented normal approximation of the binomial distribution is evaluated. There are two treatment groups \\(E\\) (experimental) and \\(C\\) (control) and the outcome of interest is the event rate in both groups. The event rate in the control group is assumed to equal \\(p_C = 0.3\\). Under the null hypothesis, the rate difference \\(p_E - p_C = 0\\) is assumed. Under the alternative, the rate difference is assumed to equal \\(p_E - p_C = 0.2\\). Type one error rate should be protected at 2.5% and the design’s power should be at least 90%. # data distribution and hypotheses datadist &lt;- Binomial(rate_control = 0.3, two_armed = TRUE) H_0 &lt;- PointMassPrior(.0, 1) prior &lt;- PointMassPrior(.2, 1) # define constraints alpha &lt;- 0.025 min_power &lt;- 0.9 toer_cnstr &lt;- Power(datadist, H_0) &lt;= alpha pow_cnstr &lt;- Power(datadist, prior) &gt;= min_power 7.2 Variant VI-1, efficiency of optimal designs for binomial distribution In this variant, it is evaluated whether the optimization leads to more efficient designs and is plausible with published results. 7.2.1 Objective Expected sample size under the alternative point mass prior \\(\\delta = 0.2\\) is minimized. ess &lt;- ExpectedSampleSize(datadist, prior) 7.2.2 Constraints No additional constraints are considered. 7.2.3 Initial Designs For this example, the optimal one-stage, group-sequential, and generic two-stage designs are computed. The initial design for the one-stage case is determined heuristically and both the group sequential and the generic two-stage designs are optimized starting from the corresponding group-sequential design as computed by the rpact package. order &lt;- 7L # data frame of initial designs tbl_designs &lt;- tibble( type = c(&quot;one-stage&quot;, &quot;group-sequential&quot;, &quot;two-stage&quot;), initial = list( OneStageDesign(200, 2.0), rpact_design(datadist, 0.2, 0.025, 0.9, TRUE, order), TwoStageDesign(rpact_design(datadist, 0.2, 0.025, 0.9, TRUE, order))) ) The order of integration is set to 7. 7.2.4 Optimization tbl_designs &lt;- tbl_designs %&gt;% mutate( optimal = purrr::map(initial, ~minimize( ess, subject_to( toer_cnstr, pow_cnstr ), initial_design = ., opts = opts)) ) 7.2.5 Test Cases To avoid improper solutions, it is first verified that the maximum number of iterations was not exceeded in any of the three cases. tbl_designs %&gt;% transmute( type, iterations = purrr::map_int(tbl_designs$optimal, ~.$nloptr_return$iterations) ) %&gt;% {print(.); .} %&gt;% {testthat::expect_true(all(.$iterations &lt; opts$maxeval))} ## # A tibble: 3 × 2 ## type iterations ## &lt;chr&gt; &lt;int&gt; ## 1 one-stage 28 ## 2 group-sequential 1774 ## 3 two-stage 4736 Next, the type one error rate and power constraints are verified for all three designs by simulation: tbl_designs %&gt;% transmute( type, toer = purrr::map(tbl_designs$optimal, ~sim_pr_reject(.[[1]], .0, datadist)$prob), power = purrr::map(tbl_designs$optimal, ~sim_pr_reject(.[[1]], .2, datadist)$prob) ) %&gt;% unnest(., cols = c(toer, power)) %&gt;% {print(.); .} %&gt;% { testthat::expect_true(all(.$toer &lt;= alpha * (1 + tol))) testthat::expect_true(all(.$power &gt;= min_power * (1 - tol))) } ## # A tibble: 3 × 3 ## type toer power ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 one-stage 0.0251 0.900 ## 2 group-sequential 0.0249 0.900 ## 3 two-stage 0.0249 0.901 Since the degrees of freedom of the three design classes are ordered as ‘two-stage’ &gt; ‘group-sequential’ &gt; ‘one-stage’, the expected sample sizes (under the alternative) should be ordered in reverse (‘two-stage’ smallest). Additionally, expected sample sizes under both null and alternative are computed both via evaluate() and simulation-based. ess0 &lt;- ExpectedSampleSize(datadist, H_0) tbl_designs %&gt;% mutate( ess = map_dbl(optimal, ~evaluate(ess, .$design) ), ess_sim = map_dbl(optimal, ~sim_n(.$design, .2, datadist)$n ), ess0 = map_dbl(optimal, ~evaluate(ess0, .$design) ), ess0_sim = map_dbl(optimal, ~sim_n(.$design, .0, datadist)$n ) ) %&gt;% {print(.); .} %&gt;% { # sim/evaluate same under alternative? testthat::expect_equal(.$ess, .$ess_sim, tolerance = tol_n, scale = 1) # sim/evaluate same under null? testthat::expect_equal(.$ess0, .$ess0_sim, tolerance = tol_n, scale = 1) # monotonicity with respect to degrees of freedom testthat::expect_true(all(diff(.$ess) &lt; 0)) } ## # A tibble: 3 × 7 ## type initial optimal ess ess_sim ess0 ess0_sim ## &lt;chr&gt; &lt;list&gt; &lt;list&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 one-stage &lt;OnStgDsg&gt; &lt;adptrOpR [3]&gt; 124 124 124 124 ## 2 group-sequential &lt;GrpSqntD&gt; &lt;adptrOpR [3]&gt; 96.2 96.2 89.7 89.8 ## 3 two-stage &lt;TwStgDsg&gt; &lt;adptrOpR [3]&gt; 94.3 94.2 98.5 98.6 The expected sample size under the alternative must be lower or equal than the expected sample size of the initial rpact group-sequential design that is based on the inverse normal combination test. testthat::expect_lte( evaluate(ess, tbl_designs %&gt;% filter(type == &quot;group-sequential&quot;) %&gt;% pull(optimal) %&gt;% .[[1]] %&gt;% .$design ), evaluate(ess, tbl_designs %&gt;% filter(type == &quot;group-sequential&quot;) %&gt;% pull(initial) %&gt;% .[[1]] ) ) Finally, the rpact group-sequential design, the optimal group-sequential design, and the optimal generic two-stage design are plotted to allow visual inspection. "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
